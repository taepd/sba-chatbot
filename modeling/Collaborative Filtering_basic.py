# -*- coding: utf-8 -*-
"""협업필터링 연습.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRv0QK8XQ-PnTkYD-TbVaeSts1yhtGJM
"""

import numpy as np
import pandas as pd
import pdb

"""- 데이터셋 로드"""

df = pd.read_csv("./ratings.dat", sep="::", encoding='utf-8')

df.head()

"""- 컬럼명 추가"""

df.columns = ["user_id", "item_id", "rating", "timestamp"]
df.head()

"""- shape 확인"""

df.shape

"""- groupby로 rating 별 user_id를 count"""

df.groupby(["rating"])[["user_id"]].count()

"""- groupby로 item_id 별 user_id를 count"""

df.groupby(["item_id"])[["user_id"]].count().head()

"""- 중복 제거한 user_id 리스트 생성
- user_id를 중복을 제거해서 count
"""

unique_user_list = df.user_id.unique()
n_users = unique_user_list.shape[0]
print(unique_user_list)
n_users

"""- item_id를 중복을 제거
   - item_id는 dense하지 않으므로 가장 큰 item_id값에 맞춰서 크기 설정
"""

unique_item_list = df.item_id.unique()
n_items = np.max(unique_item_list)
print(unique_item_list)
print(np.sort(unique_item_list))
n_items

"""- shape(n_users, n_items) ratings 영행렬의 평가행렬 생성"""

ratings = np.zeros((n_users, n_items))
ratings.shape

"""- ratings를 sparse matrix 형태로 구성
- df.itertuples(): dataframe의 각 행을 Pandas 객체로 리턴
  - 컬럼명으로도 접근 가능하고, 인덱스로도 접근 가능
  - 단, 인덱스는 0부터 시작하므로 -1씩 해준다.
"""

for row in df.itertuples():
    ratings[row[1] - 1, row[2] - 1] = row[3]

ratings

"""- scikit-learn 라이브러리를 활용해 훈련 데이터와 테스트 데이터를 분리"""

from sklearn.model_selection import train_test_split

# random_state: seed 부여 랜덤하게 만들되, 해당 시드일 경우는 항상 같음
ratings_train, ratings_test = train_test_split(ratings, test_size=0.33, random_state=37)

print(ratings_train)
ratings_train.shape

"""- 사용자 간 유사도 행렬 생성
- 코사인 유사도를 사용
  - cosine_distances는 유사할수록 값이 0에 가까움
  - cosine_similarity는 유사할수록 값이 1에 가까움
    - cosine_similarty = 1- cosine_distances
"""

from sklearn.metrics.pairwise import cosine_similarity, cosine_distances

user_similarity = cosine_similarity(ratings_train)
user_similarity

"""## 평가 예측

- 트레이닝 행렬과 사용자간 유사도 행렬간 내적연산(dot())을 함
"""

print('사용자간 유사도 행렬:', user_similarity.shape)
print('트레이닝 행렬:', ratings_train.shape)
user_similarity_dot_ratings_train = user_similarity.dot(ratings_train)

"""- 사용자간 유사도 행렬의 열(axis=1)로 즉, 아이템을 기준으로 모두 더해준 array를 생성"""

sum_by_item_id_array = np.array([np.abs(user_similarity).sum(axis=1)])
print(sum_by_item_id_array.shape)
sum_by_item_id_array

"""- 예측값을 user_similarity_dot_ratings_train에 sum_by_item_id_array를 전치하여 나누어 계산
- 즉, 유사도에 따라 가중치를 부여하여 item_id에 따른 유사도 합으로 나누어주어 예측값 생성
"""

user_pred = user_similarity_dot_ratings_train / sum_by_item_id_array.T
print(user_pred.shape)
user_pred

"""## 모델의 성능 평가

- RMSE(mean_squared_error)로 성능 평가(오차 계산)

- scikit-learn을 활용해 MSE 함수 구현 
- 예측 행렬과 트레이닝 행렬의 요소 중 0이 아닌 값들을 뽑아서 mean_squared_error()에 인자로 넣어줌
  - numpy nonzero()메소드를 이용해 트레이닝 행렬의 nonzero 요소값을 구하고, 예측 행렬과 트레이닝 행렬에서 이를 만족하는 요소들의 array를 만듬
"""

print(user_pred[ratings_train.nonzero()].shape)  # array이기 때문에 flatton()하지 않아도 됨
user_pred[ratings_train.nonzero()].shape

from sklearn.metrics import mean_squared_error


def get_mse(pred, actual):
    pred = pred[actual.nonzero()]
    actual = actual[actual.nonzero()]
    return mean_squared_error(pred, actual)


"""- RMSE 측정
- 트레이닝 오차
"""

print(np.sqrt(get_mse(user_pred, ratings_train)))

"""- 테스트 오차"""

print(np.sqrt(get_mse(user_pred, ratings_test)))

pdb.set_trace()
"""## 모든 트레이닝 세트 샘플을 대상으로 예측을 한 결과 오차가 너무 크게 나옴. 따라서 타겟과 가장 유사한 n개의 샘플을 대상을 선택하여 예측 시도

- NearestNeighbors: knn과 유사하나 비지도 방식
  - 코사인 유사도로 측정
"""

from sklearn.neighbors import NearestNeighbors

k = 100

neigh = NearestNeighbors(n_neighbors=k, metric="cosine")

neigh.fit(ratings_train)

top_k_distances, top_k_users = neigh.kneighbors(ratings_train, return_distance=True)

print(top_k_distances)
print(top_k_users)
print(top_k_distances.shape)
print(top_k_users.shape)

"""- 선택된 n명의 사용자들의 평가 가중치 합을 사용한 예측 및 모델의 성능 측정

- shape 체크 부분(흐름과 상관없음)
"""

print(np.array([np.abs(top_k_distances[1]).sum()]))
print(np.array([np.abs(top_k_distances[1].T).sum(axis=0)]).shape)
np.array([np.abs(top_k_distances[1].T).sum(axis=0)]).T.shape
print(top_k_distances[1].T.shape)
print(ratings_train[top_k_users[1]].shape)
# top_k_distances[i].T.dot(ratings_train[top_k_users][i]).shape

"""- 한 줄씩 계산하므로 벡터이기 때문에 굳이 전치하지 않아도 결과는 같다"""

# user_pred_k_1 = top_k_distances[1].dot(ratings_train[top_k_users][1]) / \
#                       np.array([np.abs(top_k_distances[1]).sum(axis=0)])
# print(user_pred_k_1)
# user_pred_k_2 = top_k_distances[1].T.dot(ratings_train[top_k_users][1]) / \
#                       np.array([np.abs(top_k_distances[1].T).sum(axis=0)]).T
# print(user_pred_k_2)
# (user_pred_k_1 == user_pred_k_2).min()

user_pred_k = np.zeros(ratings_train.shape)
user_pred_k.shape

for i in range(ratings_train.shape[0]):  # 4046: 테스트 샘플 수
    if i % 50 == 0:  # 계산이 오래 걸리므로 카운터 체크
        print("cnt = ", i)
    # top_k_distances[i].T.shape = (5, )
    # ratings_train[top_k_users[i]] = (5, 3952)
    # user_pred_k[i, :] = top_k_distances[i].T.dot(ratings_train[top_k_users][i]) / \
    #                     np.array([np.abs(top_k_distances[i].T).sum(axis=0)]).T
    user_pred_k_1 = top_k_distances[1].dot(ratings_train[top_k_users][1]) / \
                    np.array([np.abs(top_k_distances[1]).sum(axis=0)])

"""- RMSE 측정
- 트레이닝 오차
"""

np.sqrt(get_mse(user_pred_k, ratings_train))

"""- 테스트 오차"""

np.sqrt(get_mse(user_pred_k, ratings_test))

"""## 아이템 기반 협업 필터링 사용 추천 엔진

- 영화의 수를 k로 사용해 영화 간 유사도 행렬 계산
"""

k = ratings_train.shape[1]
print('k: ', k)
neigh = NearestNeighbors(n_neighbors=k, metric="cosine")

print(ratings_train.T.shape)
neigh.fit(ratings_train.T)

item_distances, _ = neigh.kneighbors(ratings_train.T, return_distance=True)

print(item_distances.shape)
print(item_distances[1, 0])
item_distances

item_pred = ratings_train.dot(item_distances) / np.array([np.abs(item_distances).sum(axis=1)])

item_pred.shape

np.sqrt(get_mse(item_pred, ratings_train))

np.sqrt(get_mse(item_pred, ratings_test))
