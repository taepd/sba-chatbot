# -*- coding: utf-8 -*-
"""sba_project_modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApXfhbPA6eUg1mcKU4D0cKkVO2ZmoidH
"""

import numpy as np
import pandas as pd

"""## 데이터 전처리

- 데이터 임포트
"""

df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/order_review(remove_userid_nan).csv", sep=",", encoding='utf-8-sig', error_bad_lines=False, engine="python")

print(df.shape)
df.head()

"""- 필요한 컬럼만 추출"""

df = df[[ 'userid', 'shop_id', 'taste_rate', 'quantity_rate', 'delivery_rate']]
df

"""- 합계 평점 컬럼 생성"""

df['rating'] = (df['taste_rate'] + df['quantity_rate'] + df['delivery_rate'])/3
df.head()

"""- 세부 평점 컬럼 제거"""

df = df[['userid', 'shop_id', 'rating']]
df.head()

"""- 정렬"""

df_sort = df.sort_values(by=['userid', 'shop_id'], axis=0)
df[df_sort['userid'] == 'user000000']

"""- 유저 리스트가 몇몇 부분 연속되어 있지 않은 것 확인"""

# unique_user_list = df.userid.unique()
# n_users = unique_user_list.shape[0]
# np.set_printoptions(threshold=np.inf)
# print(np.sort(unique_user_list))
# n_users

"""- 그룹화"""

# groupby_df = df_sort.groupby(["userid", 'shop_id'])  # groupby 객체 상태
df_group = df_sort.groupby(["userid", 'shop_id']).mean()  # groupby 객체 상태
df = df_group.reset_index()  # reset_index를 해주면 dataframe 객체가 됨
print(df.shape)
df_backup = df
df

# df = groupby_df.head()  # 이유는 모르겠지만 head를 붙였더니 dataframe화 되었음
# df

"""- 평점이 0인 항목을 제거"""

df = df.loc[(df.rating != 0)]
df

"""- 잘 변경 되었는지 확인"""

df1 = df[df['rating'] == 0]
df1

"""- shop_id가 nan인 컬럼 제거 위해 확인"""

# Check missing data
print('missing number of userid data is ', df['userid'].isnull().sum())
print('missing number of food_id data is ', df['shop_id'].isnull().sum())
print('missing number of rating data is ', df['rating'].isnull().sum())

"""- 유저 리스트 및 유저 수
    - 주의: 일부 유저 연번은 데이터 처리 과정에서 업데이트되어 리뷰 데이터가 없어졌음.
"""

unique_user_list = df.userid.unique()
n_users = unique_user_list.shape[0]
print(unique_user_list)
n_users

"""- 매장 리스트 및 매장 수 조회"""

unique_shop_list = df.shop_id.unique()
n_shops = unique_shop_list.shape[0]
print(unique_shop_list)
n_shops

"""- pivot table로 뽑아보기"""

df_table = pd.pivot_table(df, index='userid', columns='shop_id',  values='rating', fill_value='', aggfunc='first')

"""- 부분 발췌하여 희소 행렬 확인"""

df_table.iloc[808:817, 212:222]

"""### 데이터 변환(Data Transformation)
- 벡터화 연산을 위해 문자열을 숫자로 변환 필요

- 유저를 숫자로 변환
"""

# df.userid = df.userid.astype('category').cat.codes.values  # 향후 확인을 위해 이 방식보다는 map을 사용해야 함
df['userid'] = df['userid'].map(lambda x: int(x.lstrip('user')))
df

"""- id 처리 잘 되었는지 확인"""

df[(df['shop_id'] == 67) & (df['userid'] == 4637)]

"""- 유저, 아이템을 촘촘하게 배열하도록 다시 인코딩"""

# n_items = len(df.shop_id.unique()) 
# print(n_items)
# n_users = len(df.userid.unique())
# print(n_users)
# n_latent_factors = 64
# print(df.shop_id.max())
# print(df.userid.max())
# print(df.head())

users = df.userid.unique()
shops = df.shop_id.unique()

userid2idx = {o:i for i,o in enumerate(users)}
shop_id2idx = {o:i for i,o in enumerate(shops)}

df['userid'] = df['userid'].apply(lambda x: userid2idx[x])
df['shop_id'] = df['shop_id'].apply(lambda x: shop_id2idx[x])
n_items = len(df.shop_id.unique()) 
print(n_items)
n_users = len(df.userid.unique())
print(n_users)
df.head()

"""### 훈련, 테스트 데이터 분할"""

# Split train and test data
df['rating'] =  pd.to_numeric(df['rating'])  # 이상하게도 문자열로 저장되어 있어서 float로 변경
split = np.random.rand(len(df)) < 0.8
# print(split)

train_df = df[split]
test_df = df[~split]

print('shape of train data is ',train_df.shape)
print('shape of test data is ',test_df.shape)

"""### 임베딩(Embedding)

- 잠재 인수(Latent factor)의 모양(Shape)를 정의하고 모델에 삽입하는 것
- 임베딩(Embedding)된 값들은 모델이 훈련되면서 학습됨
- 손실함수(Loss function)가 최저값을 찾아가는 과정에서 최적의 값으로 자동으로 세팅
"""

from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, dot
from tensorflow.keras import regularizers
from keras import losses

# input tensor
item_input = Input(shape=[1]) # df.shop_id: 숫자로 인코딩된 벡터 형태
user_input = Input(shape=[1]) # df.userid

"""- regularizer
  - 참고: https://wdprogrammer.tistory.com/33
  - 과대적합을 피하는 처리 과정
  - L2 regularization(=weight decay) : 가중치의 제곱에 비례하는 비용이 추가됨(가중치의 L2 norm)
"""

n_latent_factors = 64
# Item latent factor
item_embedding = Embedding(n_items, n_latent_factors,  # 임베딩 입력 수가 해당 컬럼의 가장 큰 수여야 함
                            embeddings_regularizer=regularizers.l2(0.00001), 
                            name='item_embedding')(item_input)

# User latent factor
user_embedding = Embedding(n_users,  n_latent_factors,
                           embeddings_regularizer=regularizers.l2(0.00001),
                           name='user_embedding')(user_input)

"""- 벡터화 (Flatten)
  - 2D로 되어 있는 임베딩(Embedding)을 1D로 변환한다. 이것을 잠재 벡터(Latent vector) 라고 함
"""

# Item latent vector
item_vec = Flatten()(item_embedding)
# User latent vector
user_vec = Flatten()(user_embedding)
print(item_vec)

"""- 모델링(Modeling)"""

r_hat = dot([item_vec, user_vec], axes=-1)
model = Model([user_input, item_input], r_hat)
# model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
model.compile(optimizer = 'adam', loss = 'logcosh', metrics=['accuracy'])

"""- 모델 훈련(Train Model)"""

# Commented out IPython magic to ensure Python compatibility.
hist = model.fit([train_df.userid, train_df.shop_id], train_df.rating, validation_split=0.1,
                 batch_size=128, epochs=10, verbose=1) 
print(hist.history.keys())
print('train loss: ', hist.history['loss'][-1])
print('train acc: ', hist.history['accuracy'][-1])
print('val acc: ', hist.history['val_loss'][-1])
print('val acc: ', hist.history['val_accuracy'][-1])


# %matplotlib inline
import matplotlib.pyplot as plt

plt.plot(hist.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

"""- 모델 평가(Test Model)"""

test_loss = model.evaluate([test_df.userid, test_df.shop_id], test_df.rating)

print('test loss: ', test_loss)

"""- ## 학습된 머신을 활용한 예측"""

pd.options.display.float_format = '{:.2f}'.format  # 출력 포매팅 설정
# ratings_df[(ratings_df['userid'] == 249) & (ratings_df['shop_Id'] == 70)]
# print(df_backup.head(30))
print(df_backup[df_backup['userid'] == 'user000777'])
# print(df.head())

userid = 777    # 1 ~ 610
shop_id = 67
target = df_backup[(df_backup['userid'] == ('user' + f'{str(userid).zfill(6)}')) & (df_backup['shop_id'] == shop_id)]
# print('user' + str(userid).zfill(6))
user_v = np.expand_dims(userid2idx[userid], 0)
shop_v = np.expand_dims(shop_id2idx[shop_id], 0)
print(user_v)
print(shop_v)
predict = model.predict([user_v, shop_v])
if target.empty:
    print(f'실제값: 없음, 예측값: {predict[0]}')
else:
    print(f'실제값: {target.rating.values[0]}, 예측값: {predict[0]}')

"""### 모델 저장"""

from keras.models import load_model
model.save('recommender_mf.h5')